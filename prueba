###  <p align = "center"> TRABAJO DE FIN DE MÁSTER EN BIG DATA Y DATA SCIENCE. APLICACIONES AL COMERCIO, EMPRESA Y FINANZAS </p>

# <p align = "center"> **CUADRO DE MANDO PARA MONITORIZAR UNA CARTERA DE FONDOS DE INVERSION Y HACER PREDICCIONES MEDIANTE ALGORITMOS DE MACHINE LEARNING** </p>

 <p align = "left"> Presentado por: GONZALO ÁLVAREZ SÁNCHEZ </p>

<p align = "left"> Tutorizado por: CARLOS ORTEGA y SANTIAGO MOTA HERCE </p>



-------------------------------------------------------------

Para la creación de las diferentes carteras de Fondos de Inversión, las cuáles han sido previamente configuradas, se va a realizar la carga de los archivos .CSV que previamente se han descargado de la web de CAIXABANK de donde se han seleccionado diferentes fondos que podrían complementar bien nuestras carteras según el tipo de fondo que queremos incluir. Posteriormente, se analizarán y compondrán las diferentes carteras, que estarán formadas por dichos fondos en diferentes porcentajes.

Se han subido los diferentes archivos que se van a usar a un repositorio creado para la ocasión, y que se irá enumerando a medida que se importen cada fondo:

*Nota*: Antes de la carga se procederá a la importación de librerías apropiadas para esta primera parte:

# Importación librerías típicas
import pandas as pd # Con Pandas realizaremos la lectura de los difentes dataset
import numpy as np
import matplotlib.pyplot as plt # Matplotlib va a permitir el graficado de nuestros datos
from bokeh.plotting import figure, output_file, show # Bokeh nos permite graficar coronas circulares para mostrar los porcentajes de Fondo por Cartera
from itertools import accumulate # Permite realizar la suma acumulada de cifras
from datetime import datetime, timedelta

1.   Fondo de Renta Fija Euro: CaixaBank Duración Flexible 0-2, FI Clase Universal - ES0147507034

# Importación de dataset Fondo de Renta Fija Euro
FI_RF_Eur_url = 'https://raw.githubusercontent.com/gonzaloalvsan/TFM_Gonzalo/main/CaixaBank%20Duraci%C3%B3n%20Flexible%200-2%2C%20FI%20Clase%20Universal%20-%20ES0147507034.csv'

# Lectura del dataset con pandas read_csv (separador ;) 
FI_RF_Eur_df = pd.read_csv(FI_RF_Eur_url, sep = ';',decimal=',')

# Cambio formato de dato VLP de object a float y Fecha de object a datetime
FI_RF_Eur_df['VLP'] = FI_RF_Eur_df['VLP'].astype(float)
FI_RF_Eur_df['Fecha'] = pd.to_datetime(FI_RF_Eur_df['Fecha'], format="%d/%m/%Y")

# Visualización del dataset
FI_RF_Eur_df

Vamos a representarlo gráficamente. Para ello, vamos a usar conjuntamente las librerás *numpy* y *matplotlib*.

En primer lugar, vamos a importar de *matplotlib* los diferentes estilos mediante *style*. Vamos a  usar *'ggplot'*, para que nos de un gráfico de evolución.

A continuación, definiremos la variable **X** que será el conjunto de *Fechas* y la variable **Y** para los *Valores liquidativos* dada cada una de las fechas. Se trata de valores diarios.

# División entre X e Y y graficado
x,y=FI_RF_Eur_df.Fecha,FI_RF_Eur_df.VLP

from matplotlib import style

style.use('ggplot')

plt.figure(figsize=(16,8))
plt.plot(x,y)
plt.title('CaixaBank Duración Flexible 0-2, FI Clase Universal')
plt.ylabel('VLP')
plt.xlabel('Fechas')
plt.show()

A continuación, se va a calcular la rentabilidad diaria y la acumulada acumulada.

# Calculo Rentabilidad diaria
Rentabilidad_incial = FI_RF_Eur_df.loc[0,'VLP']
for i in range(1,len(FI_RF_Eur_df)):
  FI_RF_Eur_df.loc[i,'Rentabilidad_diaria'] = (((FI_RF_Eur_df.VLP[i] - FI_RF_Eur_df.VLP[i-1])/FI_RF_Eur_df.VLP[i-1])*100)

FI_RF_Eur_df.Rentabilidad_diaria[0] = 0

FI_RF_Eur_df

# Calculo Rentabilidad diaria acumulada
FI_RF_Eur_df.loc[:,'Rentabilidad'] = list(accumulate(FI_RF_Eur_df.Rentabilidad_diaria))

FI_RF_Eur_df

# Representación gráfica del Fondo de Renta Fija Euro
x,y=FI_RF_Eur_df.Fecha,FI_RF_Eur_df.Rentabilidad

plt.figure(figsize=(16,8))
plt.plot(x,y)
plt.title('CaixaBank Duración Flexible 0-2, FI Clase Universal')
plt.ylabel('Rentabilidad')
plt.xlabel('Fechas')
plt.show()



2.   Fondo de Renta Fija Internacional: CaixaBank Bonos Internacional, FI Clase Universal - ES0159178039



# Importación de dataset Fondo de Renta Fija Internacional
FI_RF_Int_url = 'https://raw.githubusercontent.com/gonzaloalvsan/TFM_Gonzalo/main/CaixaBank%20Bonos%20Internacional%2C%20FI%20Clase%20Universal%20-%20ES0159178039.csv'

# Lectura del dataset con pandas read_csv (separador ;) 
FI_RF_Int_df = pd.read_csv(FI_RF_Int_url, sep = ';',decimal=',')

# Cambio formato de dato VLP de object a float y Fecha de object a datetime
FI_RF_Int_df['VLP'] = FI_RF_Int_df['VLP'].astype(float)
FI_RF_Int_df['Fecha'] = pd.to_datetime(FI_RF_Int_df['Fecha'], format="%d/%m/%Y")

# Visualización del dataset
FI_RF_Int_df

A continuación, definiremos la variable **X** que será el conjunto de *Fechas* y la variable **Y** para los *Valores liquidativos* dada cada una de las fechas. Se trata de valores diarios.

# Representación gráfica de los VLP del Fondo Renta Fija Internacional
style.use('ggplot')

x,y=FI_RF_Int_df.Fecha,FI_RF_Int_df.VLP

plt.figure(figsize=(16,8))
plt.plot(x,y)
plt.title('CaixaBank Bonos Internacional, FI Clase Universal')
plt.ylabel('VLP')
plt.xlabel('Fechas')
plt.show()

A continuación, se va a calcular la rentabilidad diaria y la acumulada.

# Calculo Rentabilidad diaria
Rentabilidad_incial = FI_RF_Int_df.loc[0,'VLP']
for i in range(1,len(FI_RF_Int_df)):
  FI_RF_Int_df.loc[i,'Rentabilidad_diaria'] = (((FI_RF_Int_df.VLP[i] - FI_RF_Int_df.VLP[i-1])/FI_RF_Int_df.VLP[i-1])*100)

FI_RF_Int_df.Rentabilidad_diaria[0] = 0

FI_RF_Int_df

# Calculo Rentabilidad diaria acumulada
FI_RF_Int_df.loc[:,'Rentabilidad'] = list(accumulate(FI_RF_Int_df.Rentabilidad_diaria))

FI_RF_Int_df

# Representación gráfica de la rentabilidad acumulada Fondo de Renta Fija Internacional
x,y=FI_RF_Int_df.Fecha,FI_RF_Int_df.Rentabilidad

plt.figure(figsize=(16,8))
plt.plot(x,y)
plt.title('CaixaBank Bonos Internacional, FI Clase Universal')
plt.ylabel('Rentabilidad')
plt.xlabel('Fechas')
plt.show()

3.   Fondo de Gestión Alternativa: CaixaBank Gestion Activa, FI - ES0113386009

# Importación de dataset Fondo de Gestión Alternativa
FI_Gest_Alt_url = 'https://raw.githubusercontent.com/gonzaloalvsan/TFM_Gonzalo/main/CaixaBank%20Gestion%20Activa%2C%20FI%20-%20ES0113386009.csv'

# Lectura del dataset con pandas read_csv (separador ;) 
FI_Gest_Alt_df = pd.read_csv(FI_Gest_Alt_url, sep = ';',decimal=',')

# Cambio formato de dato VLP de object a float y Fecha de object a datetime
FI_Gest_Alt_df['VLP'] = FI_Gest_Alt_df['VLP'].astype(float)
FI_Gest_Alt_df['Fecha'] = pd.to_datetime(FI_Gest_Alt_df['Fecha'], format="%d/%m/%Y")

# Visualización del dataset
FI_Gest_Alt_df

A continuación, definiremos la variable **X** que será el conjunto de *Fechas* y la variable **Y** para los *Valores liquidativos* dada cada una de las fechas. Se trata de valores diarios.

# Representación gráfica de VLP de Fondo de Gestión Alternativa
x,y=FI_Gest_Alt_df.Fecha,FI_Gest_Alt_df.VLP

style.use('ggplot')

plt.figure(figsize=(16,8))
plt.plot(x,y)
plt.title('CaixaBank Gestion Activa, FI')
plt.ylabel('VLP')
plt.xlabel('Fechas')
plt.show()

A continuación, se va a calcular la rentabilidad diaria y la acumulada.

Rentabilidad_incial = FI_Gest_Alt_df.loc[0,'VLP']
for i in range(1,len(FI_Gest_Alt_df)):
  FI_Gest_Alt_df.loc[i,'Rentabilidad_diaria'] = (((FI_Gest_Alt_df.VLP[i] - FI_Gest_Alt_df.VLP[i-1])/FI_Gest_Alt_df.VLP[i-1])*100)

FI_Gest_Alt_df.Rentabilidad_diaria[0] = 0

FI_Gest_Alt_df

# Calculo Rentabilidad diaria acumulada
FI_Gest_Alt_df.loc[:,'Rentabilidad'] = list(accumulate(FI_Gest_Alt_df.Rentabilidad_diaria))

FI_Gest_Alt_df

# Representación gráfica de la rentabilidad acumulada Fondo de Renta Gestión Alternativa
x,y=FI_Gest_Alt_df.Fecha,FI_Gest_Alt_df.Rentabilidad

plt.figure(figsize=(16,8))
plt.plot(x,y)
plt.title('CaixaBank Gestion Activa, FI')
plt.ylabel('Rentabilidad')
plt.xlabel('Fechas')
plt.show()

4.   Fondo de Renta Variable Internacional: CaixaBank Renta Variable Global, FI Clase Universal - ES0159037037



# Importación de dataset Fondo de Renta Variable Internacional
FI_RV_Int_url = 'https://raw.githubusercontent.com/gonzaloalvsan/TFM_Gonzalo/main/CaixaBank%20Renta%20Variable%20Global%2C%20FI%20Clase%20Universal%20-%20ES0159037037.csv'

# Lectura del dataset con pandas read_csv (separador ;) 
FI_RV_Int_df = pd.read_csv(FI_RV_Int_url,sep = ';',decimal=',')

# Cambio formato de dato VLP de object a float y Fecha de object a datetime
FI_RV_Int_df['VLP'] = FI_RV_Int_df['VLP'].astype(float)
FI_RV_Int_df['Fecha'] = pd.to_datetime(FI_RV_Int_df['Fecha'], format="%d/%m/%Y")

# Visualización del dataset
FI_RV_Int_df

A continuación, definiremos la variable **X** que será el conjunto de *Fechas* y la variable **Y** para los *Valores liquidativos* dada cada una de las fechas. Se trata de valores diarios.

# Representación gráfica de VLP de Fondo Renta Variable Internacional
style.use('ggplot')

x,y=FI_RV_Int_df.Fecha,FI_RV_Int_df.VLP

plt.figure(figsize=(16,8))
plt.plot(x,y)
plt.title('CaixaBank Renta Variable Global, FI Clase Universal')
plt.ylabel('VLP')
plt.xlabel('Fechas')
plt.show()

A continuación, se va a calcular la rentabilidad diaria y la acumulada.

# Calculo Rentabilidad diaria
Rentabilidad_incial = FI_RV_Int_df.loc[0,'VLP']
for i in range(1,len(FI_RV_Int_df)):
  FI_RV_Int_df.loc[i,'Rentabilidad_diaria'] = (((FI_RV_Int_df.VLP[i] - FI_RV_Int_df.VLP[i-1])/FI_RV_Int_df.VLP[i-1])*100)

FI_RV_Int_df.Rentabilidad_diaria[0] = 0

FI_RV_Int_df

# Calculo Rentabilidad acumulada diaria
FI_RV_Int_df.loc[:,'Rentabilidad'] = list(accumulate(FI_RV_Int_df.Rentabilidad_diaria))

FI_RV_Int_df

# Representación gráfica de la rentabilidad acumulada Fondo de Renta Variable Internacional
x,y=FI_RV_Int_df.Fecha,FI_RV_Int_df.Rentabilidad

plt.figure(figsize=(16,8))
plt.plot(x,y)
plt.title('CaixaBank Renta Variable Global, FI Clase Universal')
plt.ylabel('Rentabilidad')
plt.xlabel('Fechas')
plt.show()

5.   Fondo de Renta Variable Global: CaixaBank Global Flexible, FI - ES0164381008

# Importación de dataset Fondo de Renta Variable Global
FI_RV_Glob_url = 'https://raw.githubusercontent.com/gonzaloalvsan/TFM_Gonzalo/main/CaixaBank%20Global%20Flexible%2C%20FI%20-%20ES0164381008.csv'

# Lectura del dataset con pandas read_csv (separador ;) 
FI_RV_Glob_df = pd.read_csv(FI_RV_Glob_url, sep = ';',decimal=',')

# Cambio formato de dato VLP de object a float y Fecha de object a datetime
FI_RV_Glob_df['VLP'] = FI_RV_Glob_df['VLP'].astype(float)
FI_RV_Glob_df['Fecha'] = pd.to_datetime(FI_RV_Glob_df['Fecha'], format="%d/%m/%Y")

# Visualización del dataset
FI_RV_Glob_df

Vamos a representarlo gráficamente. En primer lugar, vamos a importar de *matplotlib* los diferentes estilos mediante *style*. Vamos a  usar *'ggplot'*, para que nos de un gráfico de evolución.

A continuación, definiremos la variable **X** que será el conjunto de *Fechas* y la variable **Y** para los *Valores liquidativos* dada cada una de las fechas. Se trata de valores diarios.

# Representación gráfica
style.use('ggplot')

x,y=FI_RV_Glob_df.Fecha,FI_RV_Glob_df.VLP

plt.figure(figsize=(16,8))
plt.plot(x,y)
plt.title('CaixaBank Global Flexible, FI')
plt.ylabel('VLP')
plt.xlabel('Fechas')
plt.show()

A continuación, vamos a calcular la rentabilidad diaria y su acumulada

# Rentabilidad diaria
Rentabilidad_incial = FI_RV_Glob_df.loc[0,'VLP']
for i in range(1,len(FI_RV_Glob_df)):
  FI_RV_Glob_df.loc[i,'Rentabilidad_diaria'] = (((FI_RV_Glob_df.VLP[i] - FI_RV_Glob_df.VLP[i-1])/FI_RV_Glob_df.VLP[i-1])*100)

FI_RV_Glob_df.Rentabilidad_diaria[0] = 0

FI_RV_Glob_df

# Calculo Rentabilidad acumulada diaria
FI_RV_Glob_df.loc[:,'Rentabilidad'] = list(accumulate(FI_RV_Glob_df.Rentabilidad_diaria))

FI_RV_Glob_df

# Representación gráfica de la rentabildiad acumulada del fondo
x,y=FI_RV_Glob_df.Fecha,FI_RV_Glob_df.Rentabilidad

plt.figure(figsize=(20,10))
plt.plot(x,y)
plt.title('CaixaBank Global Flexible, FI')
plt.ylabel('Rentabilidad')
plt.xlabel('Fechas')
plt.show()

A continuación, vamos a graficar un resumen de la evolución de la rentabilidad acumulada de cada uno de los fondos seleccionados. Obtendremos evoluciones muy diferentes debido al riesgo asumido (más riesgo más movimiento), pero si se podrá observar que debido a diferentes situaciones que afectan a la economía, habrá un movimiento parecido en dichas evoluciones.

Para ello, en primer lugar vamos a unificar las *Rentabilidades acumuladas* de cada fondo en un mismo dataframe al que vamos a denominar **Cartera_fondos**

Cartera_fondos = pd.DataFrame(FI_RF_Eur_df,columns=['Fecha','Rentabilidad'])

Cartera_fondos = Cartera_fondos.rename(columns={'Rentabilidad':'Rentab FI RF Eur'})

Cartera_fondos['Rentab FI RF Int'] = FI_RF_Int_df.Rentabilidad

Cartera_fondos['Rentab FI Gest Alt'] = FI_Gest_Alt_df.Rentabilidad

Cartera_fondos['Rentab FI RV Int'] = FI_RV_Int_df.Rentabilidad

Cartera_fondos['Rentab FI RV Glob'] = FI_RV_Glob_df.Rentabilidad

Cartera_fondos

Graficamos

x = Cartera_fondos.Fecha
y1 = FI_RF_Eur_df.Rentabilidad
y2 = FI_RF_Int_df.Rentabilidad
y3 = FI_Gest_Alt_df.Rentabilidad
y4 = FI_RV_Int_df.Rentabilidad
y5 = FI_RV_Glob_df.Rentabilidad

colors=['orange', 'purple', 'green','red', 'pink']

plt.figure(figsize=(16,8))
plt.plot(x,y1,label="Rentab FI RF Eur")
plt.plot(x,y2,label="Rentab FI RF Int")
plt.plot(x,y3,label="Rentab FI Gest Alt")
plt.plot(x,y4,label="Rentab FI RV Int")
plt.plot(x,y5,label="Rentab FI RV Glob")

plt.title("Rentabilidad Fondos de Cartera",fontsize=15)
plt.xlabel("Fecha",fontsize=13)
plt.ylabel("Rentabilidad acumulada",fontsize=13)
plt.legend(fontsize=13)
plt.show()

Para la creación de nuestra cartera de fondos, a continuación vamos a cargar los porcentajes de cada tipo de fondo (elegidos aleatoriamente, pero intentando que sean razonables según riesgo con el tipo de cartera). Posteriormente, vamos a graficarlo para darle un aspecto más visual.

# Importación de dataset Porcentaje de fondo por cartera
Composicion_cartera_url = 'https://raw.githubusercontent.com/gonzaloalvsan/TFM_Gonzalo/main/Composici%C3%B3n%20de%20Carteras.csv'

# Lectura del dataset con pandas read_csv (separador ;) 
Composicion_cartera_df = pd.read_csv(Composicion_cartera_url, sep = ';',decimal=',')

# Visualización del dataset
Composicion_cartera_df


plt.subplots(nrows=2, ncols=2, figsize=(20,15))

plt.subplot(2,2,1)
Cartera_tranquila_grf = Composicion_cartera_df['Cartera TRANQUILA']
Tipo_fondo = Composicion_cartera_df['Tipo Fondo']
colores = ["#EE6055","#60D394","#AAF683","#FFD97D","#FF9B85"]
plt.pie(Cartera_tranquila_grf, labels=Tipo_fondo, autopct="%0.1f %%", colors=colores)
plt.title("Cartera TRANQUILA")
plt.axis("equal")

plt.subplot(2,2,2)
Cartera_equilibrada_grf = Composicion_cartera_df['Cartera EQUILIBRADA']
Tipo_fondo = Composicion_cartera_df['Tipo Fondo']
colores = ["#EE6055","#60D394","#AAF683","#FFD97D","#FF9B85"]
plt.pie(Cartera_equilibrada_grf, labels=Tipo_fondo, autopct="%0.1f %%", colors=colores)
plt.title("Cartera EQUILIBRADA")
plt.axis("equal")

plt.subplot(2,2,3)
Cartera_creciente_grf = Composicion_cartera_df['Cartera CRECIENTE']
Tipo_fondo = Composicion_cartera_df['Tipo Fondo']
colores = ["#EE6055","#60D394","#AAF683","#FFD97D","#FF9B85"]
plt.pie(Cartera_creciente_grf, labels=Tipo_fondo, autopct="%0.1f %%", colors=colores)
plt.title("Cartera CRECIENTE")
plt.axis("equal")


plt.subplot(2,2,4)
Cartera_avanzada_grf = Composicion_cartera_df['Cartera AVANZADA']
Tipo_fondo = Composicion_cartera_df['Tipo Fondo']
colores = ["#EE6055","#60D394","#AAF683","#FFD97D","#FF9B85"]
plt.pie(Cartera_avanzada_grf, labels=Tipo_fondo, autopct="%0.1f %%", colors=colores)
plt.title("Cartera AVANZADA")
plt.axis("equal")

plt.suptitle("COMPOSICION DE CARTERAS",fontsize=20)
plt.show()

Con los datos anteriores, vamos a componer las diferentes carpetas según su rentabilidad y graficaremos.



1.   **CARTERA TRANQUILA**: Cartera con muy poco riesgo (conservador) y baja rentabilidad


Carteras = pd.DataFrame(Cartera_fondos,columns=['Fecha'])

Carteras['Cartera TRANQUILA'] = ((Cartera_fondos['Rentab FI RF Eur']*((Composicion_cartera_df.iloc[0]['Cartera TRANQUILA'])/100))+
                     (Cartera_fondos['Rentab FI RF Int']*((Composicion_cartera_df.iloc[1]['Cartera TRANQUILA'])/100))+
                     (Cartera_fondos['Rentab FI Gest Alt']*((Composicion_cartera_df.iloc[2]['Cartera TRANQUILA'])/100))+
                     (Cartera_fondos['Rentab FI RV Int']*((Composicion_cartera_df.iloc[3]['Cartera TRANQUILA'])/100))+
                     (Cartera_fondos['Rentab FI RV Glob']*((Composicion_cartera_df.iloc[4]['Cartera TRANQUILA'])/100)))
Carteras['Cartera EQUILIBRADA']= ((Cartera_fondos['Rentab FI RF Eur']*((Composicion_cartera_df.iloc[0]['Cartera EQUILIBRADA'])/100))+
                     (Cartera_fondos['Rentab FI RF Int']*((Composicion_cartera_df.iloc[1]['Cartera EQUILIBRADA'])/100))+
                     (Cartera_fondos['Rentab FI Gest Alt']*((Composicion_cartera_df.iloc[2]['Cartera EQUILIBRADA'])/100))+
                     (Cartera_fondos['Rentab FI RV Int']*((Composicion_cartera_df.iloc[3]['Cartera EQUILIBRADA'])/100))+
                     (Cartera_fondos['Rentab FI RV Glob']*((Composicion_cartera_df.iloc[4]['Cartera EQUILIBRADA'])/100)))
Carteras['Cartera CRECIENTE'] = ((Cartera_fondos['Rentab FI RF Eur']*((Composicion_cartera_df.iloc[0]['Cartera CRECIENTE'])/100))+
                     (Cartera_fondos['Rentab FI RF Int']*((Composicion_cartera_df.iloc[1]['Cartera CRECIENTE'])/100))+
                     (Cartera_fondos['Rentab FI Gest Alt']*((Composicion_cartera_df.iloc[2]['Cartera CRECIENTE'])/100))+
                     (Cartera_fondos['Rentab FI RV Int']*((Composicion_cartera_df.iloc[3]['Cartera CRECIENTE'])/100))+
                     (Cartera_fondos['Rentab FI RV Glob']*((Composicion_cartera_df.iloc[4]['Cartera CRECIENTE'])/100)))
Carteras['Cartera AVANZADA'] = ((Cartera_fondos['Rentab FI RF Eur']*((Composicion_cartera_df.iloc[0]['Cartera AVANZADA'])/100))+
                     (Cartera_fondos['Rentab FI RF Int']*((Composicion_cartera_df.iloc[1]['Cartera AVANZADA'])/100))+
                     (Cartera_fondos['Rentab FI Gest Alt']*((Composicion_cartera_df.iloc[2]['Cartera AVANZADA'])/100))+
                     (Cartera_fondos['Rentab FI RV Int']*((Composicion_cartera_df.iloc[3]['Cartera AVANZADA'])/100))+
                     (Cartera_fondos['Rentab FI RV Glob']*((Composicion_cartera_df.iloc[4]['Cartera AVANZADA'])/100)))

Carteras

Representamos graficamente las rentabilidades de las carpetas según las rentabilidades diarias acumuladas conjuntas según peso de los fondos que las componen

# Representación gráfica
x = Carteras.Fecha
y1 = Carteras['Cartera TRANQUILA']
y2 = Carteras['Cartera EQUILIBRADA']
y3 = Carteras['Cartera CRECIENTE']
y4 = Carteras['Cartera AVANZADA']

colors=['orange', 'purple', 'green', 'red']

plt.figure(figsize=(20,10))
plt.plot(x,y1,label="Rentab C Tranquila")
plt.plot(x,y2,label="Rentab C Equilibrada")
plt.plot(x,y3,label="Rentab C Creciente")
plt.plot(x,y4,label="Rentab C Avanzada")


plt.title("Rentabilidad Carteras",fontsize=15)
plt.xlabel("Fecha",fontsize=13)
plt.ylabel("Rentabilidad acumulada",fontsize=13)
plt.legend(fontsize=13)
plt.show()

# **IMPLEMENTACIÓN DE ALGORITMOS DE APRENDIZAJE SUPERVISADOS**

## **MODELOS DE REGRESIÓN**

# Importación librerías típicas
from datetime import datetime
import pandas as pd # Con Pandas realizaremos la lectura de los difentes dataset
import numpy as np
import matplotlib.pyplot as plt # Matplotlib va a permitir el graficado de nuestros datos
from itertools import accumulate # Permite realizar la suma acumulada de cifras
from math import sqrt  # Permite el cálculo de raíces cuadradas
from sklearn.ensemble import RandomForestRegressor # Modelo Random Forest
from sklearn.linear_model import LinearRegression # Modelo Regresión Lineal
from sklearn.linear_model import RidgeCV # Modelo Ridge
from sklearn.linear_model import LassoCV # Modelo Lasso
from sklearn.metrics import mean_squared_error #MSE
from sklearn.metrics import mean_absolute_error #MAE
from sklearn.metrics import r2_score #R2

### **1. Fondo de Renta Fija Euro: CaixaBank Duración Flexible 0-2, FI Clase Universal**

#### *PREDICCIONES A 30 DÍAS*

# Creación de dataset a partir del principal de Renta Fija Euro, indexando la Fecha
FI_RF_Eur_df_Reg_30 = FI_RF_Eur_df
FI_RF_Eur_df_Reg_30.set_index('Fecha', inplace=True)

FI_RF_Eur_df_Reg_30.head()

FI_RF_Eur_df_Reg_30.shape

FI_RF_Eur_df_Reg_30.isnull().sum()

FI_RF_Eur_df_Reg_30.isna().sum()

FI_RF_Eur_df_Reg_30.info()

FI_RF_Eur_df_Reg_30.describe()

# Graficado inicial para visualizar los datos
FI_RF_Eur_df_Reg_30['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 30 días en el futuro
dias_futuros = 30

# Creación de nueva columna (target) desplazada 30 días más tarde
FI_RF_Eur_df_Reg_30['Prediccion']=FI_RF_Eur_df_Reg_30[['VLP']].shift(-dias_futuros)
FI_RF_Eur_df_Reg_30

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 30 días
X = np.array(FI_RF_Eur_df_Reg_30.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 30
y = np.array(FI_RF_Eur_df_Reg_30['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

# Creación de rango de fechas de dataset de testeo para incorporar al df de testeo, de cara a la grafica final
inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

# Creación de los df con las predicciones según cada regresión, indexando el listado de fechas
lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

# Creación del df indexando a los datos de testeo las columnas con las diferente predicciones
Pred_vs_orig_df = FI_RF_Eur_df_Reg_30.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

Obtenidas las diferentes predicciones, vamos a visualizarlo de manera gráfica cada predicción en general respecto de la realidad para ver cómo evolucionaría, para después visualizar la predicción específicamente en los 8 primeros meses del año 2021.

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Eur_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Eur_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Eur_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

from sklearn.metrics import mean_squared_error #MSE
from sklearn.metrics import mean_absolute_error #MAE
from sklearn.metrics import r2_score #R2
from math import sqrt

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI1_30 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI1_30 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI1_30 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI1_30 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI1_30 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI1_30 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI1_30 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI1_30 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI1_30 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI1_30 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI1_30 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI1_30 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',mse_lr_FI1_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_lr_FI1_30)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 30 días es:',mae_lr_FI1_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 30 días es:',r2_lr_FI1_30)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 30 días es:',mse_r_FI1_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_r_FI1_30)
print('El Error Absoluto Medio para el Modelo de Ridge de 30 días es:',mae_r_FI1_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 30 días es:',r2_r_FI1_30)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 30 días es:',mse_l_FI1_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_l_FI1_30)
print('El Error Absoluto Medio para el Modelo de Lasso de 30 días es:',mae_l_FI1_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 30 días es:',r2_l_FI1_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI1_30 = [[mse_lr_FI1_30,rmse_lr_FI1_30,mae_lr_FI1_30,r2_lr_FI1_30],
               [mse_r_FI1_30,rmse_r_FI1_30,mae_r_FI1_30,r2_r_FI1_30],
               [mse_l_FI1_30,rmse_l_FI1_30,mae_l_FI1_30,r2_l_FI1_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI1_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

FI_RF_Eur_df_Reg_60 = FI_RF_Eur_df

FI_RF_Eur_df_Reg_60

FI_RF_Eur_df_Reg_60.head()

FI_RF_Eur_df_Reg_60.shape

FI_RF_Eur_df_Reg_60.isnull().sum()

FI_RF_Eur_df_Reg_60.isna().sum()

FI_RF_Eur_df_Reg_60.info()

FI_RF_Eur_df_Reg_60.describe()

FI_RF_Eur_df_Reg_60['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 60 días en el futuro
dias_futuros = 60

# Creación de nueva columna (target) desplazada 60 días más tarde
FI_RF_Eur_df_Reg_60['Prediccion']=FI_RF_Eur_df_Reg_60[['VLP']].shift(-dias_futuros)
FI_RF_Eur_df_Reg_60

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 60 días
X = np.array(FI_RF_Eur_df_Reg_60.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 60
y = np.array(FI_RF_Eur_df_Reg_60['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

Pred_vs_orig_df = FI_RF_Eur_df_Reg_60.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Eur_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Eur_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Eur_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

from sklearn.metrics import mean_squared_error #MSE
from sklearn.metrics import mean_absolute_error #MAE
from sklearn.metrics import r2_score #R2
from math import sqrt

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI1_60 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI1_60 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI1_60 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI1_60 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI1_60 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI1_60 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI1_60 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI1_60 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI1_60 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI1_60 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI1_60 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI1_60 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',mse_lr_FI1_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_lr_FI1_60)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 60 días es:',mae_lr_FI1_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 60 días es:',r2_lr_FI1_60)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 60 días es:',mse_r_FI1_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_r_FI1_60)
print('El Error Absoluto Medio para el Modelo de Ridge de 60 días es:',mae_r_FI1_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 60 días es:',r2_r_FI1_60)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 60 días es:',mse_l_FI1_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_l_FI1_60)
print('El Error Absoluto Medio para el Modelo de Lasso de 60 días es:',mae_l_FI1_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 60 días es:',r2_l_FI1_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI1_60 = [[mse_lr_FI1_60,rmse_lr_FI1_60,mae_lr_FI1_60,r2_lr_FI1_60],
               [mse_r_FI1_60,rmse_r_FI1_60,mae_r_FI1_60,r2_r_FI1_60],
               [mse_l_FI1_60,rmse_l_FI1_60,mae_l_FI1_60,r2_l_FI1_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI1_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()

### **2. Fondo de Renta Fija Internacional: CaixaBank Bonos Internacional, FI Clase Universal**

#### *PREDICCIONES A 30 DÍAS*

FI_RF_Int_df_Reg_30 = FI_RF_Int_df
FI_RF_Int_df_Reg_30.set_index('Fecha', inplace=True)

FI_RF_Int_df_Reg_30.head()

FI_RF_Int_df_Reg_30.shape

FI_RF_Int_df_Reg_30.isnull().sum()

FI_RF_Int_df_Reg_30.isna().sum()

FI_RF_Int_df_Reg_30.info()

FI_RF_Int_df_Reg_30.describe()

FI_RF_Int_df_Reg_30['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 30 días en el futuro
dias_futuros = 30

# Creación de nueva columna (target) desplazada 30 días más tarde
FI_RF_Int_df_Reg_30['Prediccion']=FI_RF_Int_df_Reg_30[['VLP']].shift(-dias_futuros)
FI_RF_Int_df_Reg_30

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 30 días
X = np.array(FI_RF_Int_df_Reg_30.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 30
y = np.array(FI_RF_Int_df_Reg_30['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

Pred_vs_orig_df = FI_RF_Int_df_Reg_30.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Int_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Int_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Int_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI2_30 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI2_30 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI2_30 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI2_30 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI2_30 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI2_30 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI2_30 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI2_30 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI2_30 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI2_30 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI2_30 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI2_30 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',mse_lr_FI2_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_lr_FI2_30)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 30 días es:',mae_lr_FI2_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 30 días es:',r2_lr_FI2_30)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 30 días es:',mse_r_FI2_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_r_FI2_30)
print('El Error Absoluto Medio para el Modelo de Ridge de 30 días es:',mae_r_FI2_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 30 días es:',r2_r_FI2_30)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 30 días es:',mse_l_FI2_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_l_FI2_30)
print('El Error Absoluto Medio para el Modelo de Lasso de 30 días es:',mae_l_FI2_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 30 días es:',r2_l_FI2_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI2_30 = [[mse_lr_FI2_30,rmse_lr_FI2_30,mae_lr_FI2_30,r2_lr_FI2_30],
               [mse_r_FI2_30,rmse_r_FI2_30,mae_r_FI2_30,r2_r_FI2_30],
               [mse_l_FI2_30,rmse_l_FI2_30,mae_l_FI2_30,r2_l_FI2_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI2_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

FI_RF_Int_df_Reg_60 = FI_RF_Int_df

FI_RF_Int_df_Reg_60

FI_RF_Int_df_Reg_60.head()

FI_RF_Int_df_Reg_60.shape

FI_RF_Int_df_Reg_60.isnull().sum()

FI_RF_Int_df_Reg_60.isna().sum()

FI_RF_Int_df_Reg_60.info()

FI_RF_Int_df_Reg_60.describe()

FI_RF_Int_df_Reg_60['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 60 días en el futuro
dias_futuros = 60

# Creación de nueva columna (target) desplazada 60 días más tarde
FI_RF_Int_df_Reg_60['Prediccion']=FI_RF_Int_df_Reg_60[['VLP']].shift(-dias_futuros)
FI_RF_Int_df_Reg_60

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 60 días
X = np.array(FI_RF_Int_df_Reg_60.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 60
y = np.array(FI_RF_Int_df_Reg_60['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

Pred_vs_orig_df = FI_RF_Int_df_Reg_60.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Int_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Int_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RF_Int_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI2_60 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI2_60 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI2_60 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI2_60 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI2_60 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI2_60 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI2_60 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI2_60 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI2_60 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI2_60 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI2_60 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI2_60 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',mse_lr_FI2_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_lr_FI2_60)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 60 días es:',mae_lr_FI2_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 60 días es:',r2_lr_FI2_60)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 60 días es:',mse_r_FI2_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_r_FI2_60)
print('El Error Absoluto Medio para el Modelo de Ridge de 60 días es:',mae_r_FI2_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 60 días es:',r2_r_FI2_60)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 60 días es:',mse_l_FI2_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_l_FI2_60)
print('El Error Absoluto Medio para el Modelo de Lasso de 60 días es:',mae_l_FI2_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 60 días es:',r2_l_FI2_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI2_60 = [[mse_lr_FI2_60,rmse_lr_FI2_60,mae_lr_FI2_60,r2_lr_FI2_60],
               [mse_r_FI2_60,rmse_r_FI2_60,mae_r_FI2_60,r2_r_FI2_60],
               [mse_l_FI2_60,rmse_l_FI2_60,mae_l_FI2_60,r2_l_FI2_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI2_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()

### **3. Fondo de Gestión Alternativa: CaixaBank Gestion Activa, FI**

#### *PREDICCIONES A 30 DÍAS*

FI_Gest_Alt_df_Reg_30 = FI_Gest_Alt_df
FI_Gest_Alt_df_Reg_30.set_index('Fecha', inplace=True)

FI_Gest_Alt_df_Reg_30.head()

FI_Gest_Alt_df_Reg_30.shape

FI_Gest_Alt_df_Reg_30.isnull().sum()

FI_Gest_Alt_df_Reg_30.isna().sum()

FI_Gest_Alt_df_Reg_30.info()

FI_Gest_Alt_df_Reg_30.describe()

FI_Gest_Alt_df_Reg_30['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 30 días en el futuro
dias_futuros = 30

# Creación de nueva columna (target) desplazada 30 días más tarde
FI_Gest_Alt_df_Reg_30['Prediccion']=FI_Gest_Alt_df_Reg_30[['VLP']].shift(-dias_futuros)
FI_Gest_Alt_df_Reg_30

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 30 días
X = np.array(FI_Gest_Alt_df_Reg_30.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 30
y = np.array(FI_Gest_Alt_df_Reg_30['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

Pred_vs_orig_df = FI_Gest_Alt_df_Reg_30.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_Gest_Alt_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_Gest_Alt_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_Gest_Alt_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI3_30 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI3_30 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI3_30 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI3_30 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI3_30 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI3_30 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI3_30 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI3_30 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI3_30 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI3_30 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI3_30 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI3_30 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',mse_lr_FI3_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_lr_FI3_30)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 30 días es:',mae_lr_FI3_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 30 días es:',r2_lr_FI3_30)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 30 días es:',mse_r_FI3_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_r_FI3_30)
print('El Error Absoluto Medio para el Modelo de Ridge de 30 días es:',mae_r_FI3_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 30 días es:',r2_r_FI3_30)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 30 días es:',mse_l_FI3_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_l_FI3_30)
print('El Error Absoluto Medio para el Modelo de Lasso de 30 días es:',mae_l_FI3_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 30 días es:',r2_l_FI3_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI3_30 = [[mse_lr_FI3_30,rmse_lr_FI3_30,mae_lr_FI3_30,r2_lr_FI3_30],
               [mse_r_FI3_30,rmse_r_FI3_30,mae_r_FI3_30,r2_r_FI3_30],
               [mse_l_FI3_30,rmse_l_FI3_30,mae_l_FI3_30,r2_l_FI3_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI3_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

FI_Gest_Alt_df_Reg_60 = FI_Gest_Alt_df

FI_Gest_Alt_df_Reg_60

FI_Gest_Alt_df_Reg_60.head()

FI_Gest_Alt_df_Reg_60.shape

FI_Gest_Alt_df_Reg_60.isnull().sum()

FI_Gest_Alt_df_Reg_60.isna().sum()

FI_Gest_Alt_df_Reg_60.info()

FI_Gest_Alt_df_Reg_60.describe()

FI_Gest_Alt_df_Reg_60['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 60 días en el futuro
dias_futuros = 60

# Creación de nueva columna (target) desplazada 60 días más tarde
FI_Gest_Alt_df_Reg_60['Prediccion']=FI_Gest_Alt_df_Reg_60[['VLP']].shift(-dias_futuros)
FI_Gest_Alt_df_Reg_60

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 60 días
X = np.array(FI_Gest_Alt_df_Reg_60.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 60
y = np.array(FI_Gest_Alt_df_Reg_60['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

Pred_vs_orig_df = FI_Gest_Alt_df_Reg_60.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_Gest_Alt_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_Gest_Alt_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_Gest_Alt_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI3_60 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI3_60 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI3_60 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI3_60 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI3_60 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI3_60 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI3_60 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI3_60 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI3_60 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI3_60 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI3_60 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI3_60 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',mse_lr_FI3_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_lr_FI3_60)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 60 días es:',mae_lr_FI3_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 60 días es:',r2_lr_FI3_60)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 60 días es:',mse_r_FI3_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_r_FI3_60)
print('El Error Absoluto Medio para el Modelo de Ridge de 60 días es:',mae_r_FI3_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 60 días es:',r2_r_FI3_60)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 60 días es:',mse_l_FI3_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_l_FI3_60)
print('El Error Absoluto Medio para el Modelo de Lasso de 60 días es:',mae_l_FI3_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 60 días es:',r2_l_FI3_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI3_60 = [[mse_lr_FI3_60,rmse_lr_FI3_60,mae_lr_FI3_60,r2_lr_FI3_60],
               [mse_r_FI3_60,rmse_r_FI3_60,mae_r_FI3_60,r2_r_FI3_60],
               [mse_l_FI3_60,rmse_l_FI3_60,mae_l_FI3_60,r2_l_FI3_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI3_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()

### **4. Fondo de Renta Variable Internacional: CaixaBank Renta Variable Global, FI Clase Universal**

#### *PREDICCIONES A 30 DÍAS*

FI_RV_Int_df_Reg_30 = FI_RV_Int_df
FI_RV_Int_df_Reg_30.set_index('Fecha', inplace=True)

FI_RV_Int_df_Reg_30.head()

FI_RV_Int_df_Reg_30.shape

FI_RV_Int_df_Reg_30.isnull().sum()

FI_RV_Int_df_Reg_30.isna().sum()

FI_RV_Int_df_Reg_30.info()

FI_RV_Int_df_Reg_30.describe()

FI_RV_Int_df_Reg_30['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 30 días en el futuro
dias_futuros = 30

# Creación de nueva columna (target) desplazada 30 días más tarde
FI_RV_Int_df_Reg_30['Prediccion']=FI_RV_Int_df_Reg_30[['VLP']].shift(-dias_futuros)
FI_RV_Int_df_Reg_30

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 30 días
X = np.array(FI_RV_Int_df_Reg_30.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 30
y = np.array(FI_RV_Int_df_Reg_30['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

Pred_vs_orig_df = FI_RV_Int_df_Reg_30.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Int_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Int_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Int_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI4_30 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI4_30 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI4_30 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI4_30 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI4_30 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI4_30 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI4_30 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI4_30 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI4_30 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI4_30 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI4_30 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI4_30 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',mse_lr_FI4_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_lr_FI4_30)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 30 días es:',mae_lr_FI4_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 30 días es:',r2_lr_FI4_30)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 30 días es:',mse_r_FI4_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_r_FI4_30)
print('El Error Absoluto Medio para el Modelo de Ridge de 30 días es:',mae_r_FI4_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 30 días es:',r2_r_FI4_30)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 30 días es:',mse_l_FI4_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_l_FI4_30)
print('El Error Absoluto Medio para el Modelo de Lasso de 30 días es:',mae_l_FI4_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 30 días es:',r2_l_FI4_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI4_30 = [[mse_lr_FI4_30,rmse_lr_FI4_30,mae_lr_FI4_30,r2_lr_FI4_30],
               [mse_r_FI4_30,rmse_r_FI4_30,mae_r_FI4_30,r2_r_FI4_30],
               [mse_l_FI4_30,rmse_l_FI4_30,mae_l_FI4_30,r2_l_FI4_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI4_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

FI_RV_Int_df_Reg_60 = FI_RV_Int_df

FI_RV_Int_df_Reg_60

FI_RV_Int_df_Reg_60.head()

FI_RV_Int_df_Reg_60.shape

FI_RV_Int_df_Reg_60.isnull().sum()

FI_RV_Int_df_Reg_60.isna().sum()

FI_RV_Int_df_Reg_60.info()

FI_RV_Int_df_Reg_60.describe()

FI_RV_Int_df_Reg_60['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 60 días en el futuro
dias_futuros = 60

# Creación de nueva columna (target) desplazada 60 días más tarde
FI_RV_Int_df_Reg_60['Prediccion']=FI_RV_Int_df_Reg_60[['VLP']].shift(-dias_futuros)
FI_RV_Int_df_Reg_60

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 60 días
X = np.array(FI_RV_Int_df_Reg_60.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 60
y = np.array(FI_RV_Int_df_Reg_60['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

Pred_vs_orig_df = FI_RV_Int_df_Reg_60.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Int_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Int_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Int_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI4_60 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI4_60 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI4_60 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI4_60 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI4_60 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI4_60 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI4_60 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI4_60 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI4_60 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI4_60 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI4_60 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI4_60 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',mse_lr_FI4_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_lr_FI4_60)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 60 días es:',mae_lr_FI4_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 60 días es:',r2_lr_FI4_60)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 60 días es:',mse_r_FI4_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_r_FI4_60)
print('El Error Absoluto Medio para el Modelo de Ridge de 60 días es:',mae_r_FI4_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 60 días es:',r2_r_FI4_60)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 60 días es:',mse_l_FI4_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_l_FI4_60)
print('El Error Absoluto Medio para el Modelo de Lasso de 60 días es:',mae_l_FI4_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 60 días es:',r2_l_FI4_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI4_60 = [[mse_lr_FI4_60,rmse_lr_FI4_60,mae_lr_FI4_60,r2_lr_FI4_60],
               [mse_r_FI4_60,rmse_r_FI4_60,mae_r_FI4_60,r2_r_FI4_60],
               [mse_l_FI4_60,rmse_l_FI4_60,mae_l_FI4_60,r2_l_FI4_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI4_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()

### **5. Fondo de Renta Variable Global: CaixaBank Global Flexible, FI**

#### *PREDICCIONES A 30 DÍAS*

FI_RV_Glob_df_Reg_30 = FI_RV_Glob_df
FI_RV_Glob_df_Reg_30.set_index('Fecha', inplace=True)

FI_RV_Glob_df_Reg_30.head()

FI_RV_Glob_df_Reg_30.shape

FI_RV_Glob_df_Reg_30.isnull().sum()

FI_RV_Glob_df_Reg_30.isna().sum()

FI_RV_Glob_df_Reg_30.info()

FI_RV_Glob_df_Reg_30.describe()

FI_RV_Glob_df_Reg_30['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 30 días en el futuro
dias_futuros = 30

# Creación de nueva columna (target) desplazada 30 días más tarde
FI_RV_Glob_df_Reg_30['Prediccion']=FI_RV_Glob_df_Reg_30[['VLP']].shift(-dias_futuros)
FI_RV_Glob_df_Reg_30

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 30 días
X = np.array(FI_RV_Glob_df_Reg_30.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 30
y = np.array(FI_RV_Glob_df_Reg_30['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

Pred_vs_orig_df = FI_RV_Glob_df_Reg_30.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Glob_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Glob_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Glob_df_Reg_30['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI5_30 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI5_30 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI5_30 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI5_30 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI5_30 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI5_30 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI5_30 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI5_30 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI5_30 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI5_30 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI5_30 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI5_30 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',mse_lr_FI5_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_lr_FI5_30)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 30 días es:',mae_lr_FI5_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 30 días es:',r2_lr_FI5_30)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 30 días es:',mse_r_FI5_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_r_FI5_30)
print('El Error Absoluto Medio para el Modelo de Ridge de 30 días es:',mae_r_FI5_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 30 días es:',r2_r_FI5_30)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 30 días es:',mse_l_FI5_30)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 30 días es:',rmse_l_FI5_30)
print('El Error Absoluto Medio para el Modelo de Lasso de 30 días es:',mae_l_FI5_30)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 30 días es:',r2_l_FI5_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI5_30 = [[mse_lr_FI5_30,rmse_lr_FI5_30,mae_lr_FI5_30,r2_lr_FI5_30],
               [mse_r_FI5_30,rmse_r_FI5_30,mae_r_FI5_30,r2_r_FI5_30],
               [mse_l_FI5_30,rmse_l_FI5_30,mae_l_FI5_30,r2_l_FI5_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI5_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

FI_RV_Glob_df_Reg_60 = FI_RV_Glob_df

FI_RV_Glob_df_Reg_60

FI_RV_Glob_df_Reg_60.head()

FI_RV_Glob_df_Reg_60.shape

FI_RV_Glob_df_Reg_60.isnull().sum()

FI_RV_Glob_df_Reg_60.isna().sum()

FI_RV_Glob_df_Reg_60.info()

FI_RV_Glob_df_Reg_60.describe()

FI_RV_Glob_df_Reg_60['VLP'].plot(figsize=(16,8))

# Creación nueva variable para predecir 60 días en el futuro
dias_futuros = 60

# Creación de nueva columna (target) desplazada 60 días más tarde
FI_RV_Glob_df_Reg_60['Prediccion']=FI_RV_Glob_df_Reg_60[['VLP']].shift(-dias_futuros)
FI_RV_Glob_df_Reg_60

# Creación del del futuro dataset X, convertirlo en array con numpy y eliminar los últimos 60 días
X = np.array(FI_RV_Glob_df_Reg_60.drop(['Rentabilidad_diaria','Rentabilidad','Prediccion'],1))

# Creación del dataset objetivo (y), convertirlo en array con numpy y conseguir todos los valores objetivo excepto los últimos 60
y = np.array(FI_RV_Glob_df_Reg_60['Prediccion'])

# División de datasets X e y en entrenamiento (train) y validación (test)
X_train,X_test,y_train,y_test=X[:1096],X[1096:],y[:1096],y[1096:]

# Creación de modelos

# Creación modelo Regresión lineal
lin_model=LinearRegression().fit(X_train,y_train)

# Creación modelo Ridge
R_model = RidgeCV(
              alphas = np.logspace(-10, 2, 200),
              fit_intercept   = True,
              normalize       = True,
              store_cv_values = True
          ).fit(X_train,y_train)

# Creación modelo de Lasso
lasso_model = LassoCV(
              alphas = np.logspace(-10, 3, 200),
              normalize = True,
              cv = 10
         ).fit(X_train,y_train)

# Predicción Modelo Regresión Lineal
lin_pred=lin_model.predict(X_test)
print(lin_pred)
print()

# Predicción Modelo Ridge
R_pred=R_model.predict(X_test)
print(R_pred)
print()

# Predicción Modelo Lasso
Lasso_pred=lasso_model.predict(X_test)
print(Lasso_pred)
print()

inicio = datetime(2021,1,1)
fin    = datetime(2021,8,31)

lista_fechas = [(inicio + timedelta(days=d)).strftime("%Y-%m-%d")
                    for d in range((fin - inicio).days + 1)] 

lin_pred_df = pd.DataFrame(lin_pred, index=lista_fechas, columns=['Prediccion_Lineal'])
R_pred_df = pd.DataFrame(R_pred, index=lista_fechas, columns=['Prediccion_Ridge'])
Lasso_pred_df = pd.DataFrame(Lasso_pred, index=lista_fechas, columns=['Prediccion_Lasso'])

lin_pred_df

Pred_vs_orig_df = FI_RV_Glob_df_Reg_60.merge(lin_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(R_pred_df, how='outer', left_index=True, right_index=True)
Pred_vs_orig_df = Pred_vs_orig_df.merge(Lasso_pred_df, how='outer', left_index=True, right_index=True)

Pred_vs_orig_df = Pred_vs_orig_df[1096:]
Pred_vs_orig_df

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Glob_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lineal Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lineal']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Glob_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Ridge Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Ridge']])
plt.legend(['Valid','Pred'])
plt.show()

# Visualización de datos general
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(FI_RV_Glob_df_Reg_60['VLP'][:1096])
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Orig','Valid','Pred'])
plt.show()

# Visualización de datos prediccion
plt.figure(figsize=(16,8))
plt.title('Modelo Regresión Lasso Prediccion')
plt.xlabel('Dias')
plt.ylabel('VLP')
plt.plot(Pred_vs_orig_df[['VLP','Prediccion_Lasso']])
plt.legend(['Valid','Pred'])
plt.show()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada regresión

# Calculo indicadores Regresión Lineal
mse_lr_FI5_60 = round(mean_squared_error(X_test,lin_pred),5)
rmse_lr_FI5_60 = round(sqrt(mean_squared_error(X_test,lin_pred)),5)
mae_lr_FI5_60 = round(mean_absolute_error(X_test,lin_pred),5)
r2_lr_FI5_60 = round(r2_score(X_test,lin_pred),5)

# Calculo indicadores Ridge
mse_r_FI5_60 = round(mean_squared_error(X_test,R_pred),5)
rmse_r_FI5_60 = round(sqrt(mean_squared_error(X_test,R_pred)),5)
mae_r_FI5_60 = round(mean_absolute_error(X_test,R_pred),5)
r2_r_FI5_60 = round(r2_score(X_test,R_pred),5)

# Calculo indicadores Lasso
mse_l_FI5_60 = round(mean_squared_error(X_test,Lasso_pred),5)
rmse_l_FI5_60 = round(sqrt(mean_squared_error(X_test,Lasso_pred)),5)
mae_l_FI5_60 = round(mean_absolute_error(X_test,Lasso_pred),5)
r2_l_FI5_60 = round(r2_score(X_test,Lasso_pred),5)

# Print de resultados

# Resultados Regresión Lineal
print('El Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',mse_lr_FI5_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_lr_FI5_60)
print('El Error Absoluto Medio para el Modelo de Regresión Lineal de 60 días es:',mae_lr_FI5_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Regresión Lineal de 60 días es:',r2_lr_FI5_60)

# Resultados Ridge
print('El Error Cuadrático Medio para el Modelo de Ridge de 60 días es:',mse_r_FI5_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_r_FI5_60)
print('El Error Absoluto Medio para el Modelo de Ridge de 60 días es:',mae_r_FI5_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Ridge de 60 días es:',r2_r_FI5_60)

#Resultados Lasso
print('El Error Cuadrático Medio para el Modelo de Lasso de 60 días es:',mse_l_FI5_60)
print('La Raíz de Error Cuadrático Medio para el Modelo de Regresión Lineal de 60 días es:',rmse_l_FI5_60)
print('El Error Absoluto Medio para el Modelo de Lasso de 60 días es:',mae_l_FI5_60)
print('El Coeficiente de Determinación (R2) para el Modelo de Lasso de 60 días es:',r2_l_FI5_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI5_60 = [[mse_lr_FI5_60,rmse_lr_FI5_60,mae_lr_FI5_60,r2_lr_FI5_60],
               [mse_r_FI5_60,rmse_r_FI5_60,mae_r_FI5_60,r2_r_FI5_60],
               [mse_l_FI5_60,rmse_l_FI5_60,mae_l_FI5_60,r2_l_FI5_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI5_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Regresión Lineal","Ridge","Lasso"],
                      rowColours =["yellow"] * 3,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(10)
the_table.scale(1.5,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()

## **REDES NEURONALES RECURRENTES (RNN) - LONG SHORT TERM MEMORY (LSTM)**

# Importación librerías típicas
from sklearn.preprocessing import MinMaxScaler # Se usa para la normalizacion
from keras.models import Sequential # Para iniciar la red
from keras.layers import Dense
from keras.layers import LSTM # Para agregar la capa que hace la funcion LSTM
from keras.layers import Dropout # Para evitar overfitting

### **1. Fondo de Renta Fija Euro: CaixaBank Duración Flexible 0-2, FI Clase Universal**

#### *PREDICCIONES A 30 DÍAS*

# Creación de dataset a partir del principal de Renta Fija Euro, indexando la Fecha
FI_RF_Eur_df_RNN_30 = FI_RF_Eur_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_RF_Eur_df.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 30 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 30 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 30 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 30 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 30 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 30 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(30,1096):
  X_train.append(training_set_escalado[i-30:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 30, todos los valores a la derecha son los valores de los 30 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 30 valores del train
test_set = training_set[-30:]
test_set = np.append(test_set ,FI_RF_Eur_df_RNN_30.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_RF_Eur_df_RNN_30.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(30, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-30:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la prediccion con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_RF_Eur_df_RNN_30.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Duración Flexible 0-2 30 días vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Duración Flexible 0-2 30 días vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Duración Flexible 0-2 30 días vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Duración Flexible 0-2 30 días vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI1_30 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI1_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI1_30 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI1_30 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI1_30 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI1_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI1_30 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI1_30 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI1_30 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI1_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI1_30 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI1_30 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI1_30 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI1_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI1_30 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI1_30 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mse_a100_FI1_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',rmse_a100_FI1_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mae_a100_FI1_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 30 días es:',r2_a100_FI1_30)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mse_a120_FI1_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',rmse_a120_FI1_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mae_a120_FI1_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 30 días es:',r2_a120_FI1_30)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mse_rms100_FI1_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',rmse_rms100_FI1_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mae_rms100_FI1_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',r2_rms100_FI1_30)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mse_rms120_FI1_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',rmse_rms120_FI1_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mae_rms120_FI1_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',r2_rms120_FI1_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI1_30 = [[mse_a100_FI1_30,rmse_a100_FI1_30,mae_a100_FI1_30,r2_a100_FI1_30],
               [mse_a120_FI1_30,rmse_a120_FI1_30,mae_a120_FI1_30,r2_a120_FI1_30],
               [mse_rms100_FI1_30,rmse_rms100_FI1_30,mae_rms100_FI1_30,r2_rms100_FI1_30],
               [mse_rms120_FI1_30,rmse_rms120_FI1_30,mae_rms120_FI1_30,r2_rms120_FI1_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI1_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

# Creación de dataset a partir del principal de Renta Fija Euro, indexando la Fecha
FI_RF_Eur_df_RNN_60 = FI_RF_Eur_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_RF_Eur_df.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 60 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 60 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 60 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 60 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 60 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 60 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(60,1096):
  X_train.append(training_set_escalado[i-60:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 60, todos los valores a la derecha son los valores de los 60 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 60 valores del train
test_set = training_set[-60:]
test_set = np.append(test_set ,FI_RF_Eur_df_RNN_60.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_RF_Eur_df_RNN_60.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(60, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-60:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la prediccion con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_RF_Eur_df_RNN_60.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Duración Flexible 0-2 60 días vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Duración Flexible 0-2 60 días vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Duración Flexible 0-2 60 días vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Duración Flexible 0-2 60 días vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI1_60 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI1_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI1_60 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI1_60 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI1_60 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI1_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI1_60 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI1_60 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI1_60 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI1_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI1_60 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI1_60 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI1_60 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI1_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI1_60 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI1_60 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mse_a100_FI1_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',rmse_a100_FI1_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mae_a100_FI1_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 60 días es:',r2_a100_FI1_60)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mse_a120_FI1_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',rmse_a120_FI1_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mae_a120_FI1_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 60 días es:',r2_a120_FI1_60)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mse_rms100_FI1_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',rmse_rms100_FI1_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mae_rms100_FI1_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',r2_rms100_FI1_60)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mse_rms120_FI1_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',rmse_rms120_FI1_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mae_rms120_FI1_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',r2_rms120_FI1_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI1_60 = [[mse_a100_FI1_60,rmse_a100_FI1_60,mae_a100_FI1_60,r2_a100_FI1_60],
               [mse_a120_FI1_60,rmse_a120_FI1_60,mae_a120_FI1_60,r2_a120_FI1_60],
               [mse_rms100_FI1_60,rmse_rms100_FI1_60,mae_rms100_FI1_60,r2_rms100_FI1_60],
               [mse_rms120_FI1_60,rmse_rms120_FI1_60,mae_rms120_FI1_60,r2_rms120_FI1_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI1_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()

### **2. Fondo de Renta Fija Internacional: CaixaBank Bonos Internacional, FI Clase Universal**

#### *PREDICCIONES A 30 DÍAS*

# Creación de dataset a partir del principal de Renta Fija Internacional, indexando la Fecha
FI_RF_Int_df_RNN_30 = FI_RF_Int_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_RF_Int_df.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 30 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 30 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 30 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 30 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 30 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 30 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(30,1096):
  X_train.append(training_set_escalado[i-30:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 30, todos los valores a la derecha son los valores de los 30 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 30 valores del train
test_set = training_set[-30:]
test_set = np.append(test_set ,FI_RF_Int_df_RNN_30.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_RF_Int_df_RNN_30.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(30, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-30:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la predicción con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_RF_Int_df_RNN_30.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Bonos Internacional 30 días vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Bonos Internacional 30 días vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Bonos Internacional 30 días vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Bonos Internacional 30 días vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI2_30 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI2_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI2_30 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI2_30 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI2_30 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI2_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI2_30 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI2_30 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI2_30 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI2_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI2_30 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI2_30 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI2_30 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI2_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI2_30 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI2_30 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mse_a100_FI2_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',rmse_a100_FI2_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mae_a100_FI2_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 30 días es:',r2_a100_FI2_30)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mse_a120_FI2_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',rmse_a120_FI2_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mae_a120_FI2_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 30 días es:',r2_a120_FI2_30)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mse_rms100_FI2_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',rmse_rms100_FI2_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mae_rms100_FI2_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',r2_rms100_FI2_30)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mse_rms120_FI2_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',rmse_rms120_FI2_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mae_rms120_FI2_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',r2_rms120_FI2_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI2_30 = [[mse_a100_FI2_30,rmse_a100_FI2_30,mae_a100_FI2_30,r2_a100_FI2_30],
               [mse_a120_FI2_30,rmse_a120_FI2_30,mae_a120_FI2_30,r2_a120_FI2_30],
               [mse_rms100_FI2_30,rmse_rms100_FI2_30,mae_rms100_FI2_30,r2_rms100_FI2_30],
               [mse_rms120_FI2_30,rmse_rms120_FI2_30,mae_rms120_FI2_30,r2_rms120_FI2_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI2_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

# Creación de dataset a partir del principal de Renta Fija Euro, indexando la Fecha
FI_RF_Int_df_RNN_60 = FI_RF_Int_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_RF_Int_df.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 60 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 60 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 60 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 60 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 60 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 60 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(60,1096):
  X_train.append(training_set_escalado[i-60:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 60, todos los valores a la derecha son los valores de los 60 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 60 valores del train
test_set = training_set[-60:]
test_set = np.append(test_set ,FI_RF_Int_df_RNN_60.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_RF_Int_df_RNN_60.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(60, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-60:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la prediccion con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_RF_Int_df_RNN_60.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Bonos Internacional 60 días vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Bonos Internacional 60 días vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Bonos Internacional 60 días vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Bonos Internacional 60 días vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI2_60 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI2_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI2_60 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI2_60 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI2_60 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI2_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI2_60 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI2_60 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI2_60 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI2_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI2_60 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI2_60 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI2_60 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI2_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI2_60 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI2_60 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mse_a100_FI2_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',rmse_a100_FI2_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mae_a100_FI2_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 60 días es:',r2_a100_FI2_60)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mse_a120_FI2_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',rmse_a120_FI2_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mae_a120_FI2_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 60 días es:',r2_a120_FI2_60)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mse_rms100_FI2_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',rmse_rms100_FI2_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mae_rms100_FI2_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',r2_rms100_FI2_60)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mse_rms120_FI2_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',rmse_rms120_FI2_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mae_rms120_FI2_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',r2_rms120_FI2_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI2_60 = [[mse_a100_FI2_60,rmse_a100_FI2_60,mae_a100_FI2_60,r2_a100_FI2_60],
               [mse_a120_FI2_60,rmse_a120_FI2_60,mae_a120_FI2_60,r2_a120_FI2_60],
               [mse_rms100_FI2_60,rmse_rms100_FI2_60,mae_rms100_FI2_60,r2_rms100_FI2_60],
               [mse_rms120_FI2_60,rmse_rms120_FI2_60,mae_rms120_FI2_60,r2_rms120_FI2_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI2_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()

### **3. Fondo de Gestión Alternativa: CaixaBank Gestion Activa, FI**

#### *PREDICCIONES A 30 DÍAS*

# Creación de dataset a partir del principal de Gestión Alternativa, indexando la Fecha
FI_Gest_Alt_df_RNN_30 = FI_Gest_Alt_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_Gest_Alt_df.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 30 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 30 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 30 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 30 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 30 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 30 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(30,1096):
  X_train.append(training_set_escalado[i-30:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 30, todos los valores a la derecha son los valores de los 30 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 30 valores del train
test_set = training_set[-30:]
test_set = np.append(test_set ,FI_Gest_Alt_df_RNN_30.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_Gest_Alt_df_RNN_30.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(30, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-30:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la predicción con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_Gest_Alt_df_RNN_30.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Gestión Activa vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Gestión Activa vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Gestión Activa vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Gestión Activa vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI3_30 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI3_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI3_30 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI3_30 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI3_30 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI3_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI3_30 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI3_30 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI3_30 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI3_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI3_30 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI3_30 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI3_30 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI3_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI3_30 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI3_30 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mse_a100_FI3_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',rmse_a100_FI3_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mae_a100_FI3_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 30 días es:',r2_a100_FI3_30)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mse_a120_FI3_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',rmse_a120_FI3_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mae_a120_FI3_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 30 días es:',r2_a120_FI3_30)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mse_rms100_FI3_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',rmse_rms100_FI3_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mae_rms100_FI3_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',r2_rms100_FI3_30)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mse_rms120_FI3_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',rmse_rms120_FI3_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mae_rms120_FI3_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',r2_rms120_FI3_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI3_30 = [[mse_a100_FI3_30,rmse_a100_FI3_30,mae_a100_FI3_30,r2_a100_FI3_30],
               [mse_a120_FI3_30,rmse_a120_FI3_30,mae_a120_FI3_30,r2_a120_FI3_30],
               [mse_rms100_FI3_30,rmse_rms100_FI3_30,mae_rms100_FI3_30,r2_rms100_FI3_30],
               [mse_rms120_FI3_30,rmse_rms120_FI3_30,mae_rms120_FI3_30,r2_rms120_FI3_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI3_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

# Creación de dataset a partir del principal de Gestión Alternativa, indexando la Fecha
FI_Gest_Alt_df_RNN_60 = FI_Gest_Alt_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_Gest_Alt_df.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 60 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 60 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 60 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 60 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 60 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 60 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(60,1096):
  X_train.append(training_set_escalado[i-60:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 60, todos los valores a la derecha son los valores de los 60 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 60 valores del train
test_set = training_set[-60:]
test_set = np.append(test_set ,FI_Gest_Alt_df_RNN_60.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_Gest_Alt_df_RNN_60.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(60, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-60:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la prediccion con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_Gest_Alt_df_RNN_60.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Gestión Activa 60 días vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Gestión Activa 60 días vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Gestión Activa 60 días vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Gestión Activa 60 días vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI3_60 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI3_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI3_60 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI3_60 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI3_60 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI3_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI3_60 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI3_60 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI3_60 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI3_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI3_60 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI3_60 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI3_60 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI3_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI3_60 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI3_60 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mse_a100_FI3_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',rmse_a100_FI3_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mae_a100_FI3_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 60 días es:',r2_a100_FI3_60)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mse_a120_FI3_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',rmse_a120_FI3_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mae_a120_FI3_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 60 días es:',r2_a120_FI3_60)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mse_rms100_FI3_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',rmse_rms100_FI3_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mae_rms100_FI3_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',r2_rms100_FI3_60)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mse_rms120_FI3_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',rmse_rms120_FI3_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mae_rms120_FI3_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',r2_rms120_FI3_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI3_60 = [[mse_a100_FI3_60,rmse_a100_FI3_60,mae_a100_FI3_60,r2_a100_FI3_60],
               [mse_a120_FI3_60,rmse_a120_FI3_60,mae_a120_FI3_60,r2_a120_FI3_60],
               [mse_rms100_FI3_60,rmse_rms100_FI3_60,mae_rms100_FI3_60,r2_rms100_FI3_60],
               [mse_rms120_FI3_60,rmse_rms120_FI3_60,mae_rms120_FI3_60,r2_rms120_FI3_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI3_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()

### **4. Fondo de Renta Variable Internacional: CaixaBank Renta Variable Global, FI Clase Universal**

#### *PREDICCIONES A 30 DÍAS*

# Creación de dataset a partir del principal de Renta Variable Internacional, indexando la Fecha
FI_RV_Int_df_RNN_30 = FI_RV_Int_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_RV_Int_df_RNN_30.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 30 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 30 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 30 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 30 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 30 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 30 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(30,1096):
  X_train.append(training_set_escalado[i-30:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 30, todos los valores a la derecha son los valores de los 30 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 30 valores del train
test_set = training_set[-30:]
test_set = np.append(test_set ,FI_RV_Int_df_RNN_30.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_RV_Int_df_RNN_30.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(30, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-30:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la predicción con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_RV_Int_df_RNN_30.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Renta Variable Global vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Renta Variable Global vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Renta Variable Global vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Renta Variable Global vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI4_30 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI4_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI4_30 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI4_30 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI4_30 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI4_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI4_30 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI4_30 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI4_30 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI4_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI4_30 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI4_30 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI4_30 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI4_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI4_30 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI4_30 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mse_a100_FI4_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',rmse_a100_FI4_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mae_a100_FI4_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 30 días es:',r2_a100_FI4_30)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mse_a120_FI4_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',rmse_a120_FI4_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mae_a120_FI4_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 30 días es:',r2_a120_FI4_30)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mse_rms100_FI4_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',rmse_rms100_FI4_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mae_rms100_FI4_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',r2_rms100_FI4_30)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mse_rms120_FI4_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',rmse_rms120_FI4_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mae_rms120_FI4_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',r2_rms120_FI4_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI4_30 = [[mse_a100_FI4_30,rmse_a100_FI4_30,mae_a100_FI4_30,r2_a100_FI4_30],
               [mse_a120_FI4_30,rmse_a120_FI4_30,mae_a120_FI4_30,r2_a120_FI4_30],
               [mse_rms100_FI4_30,rmse_rms100_FI4_30,mae_rms100_FI4_30,r2_rms100_FI4_30],
               [mse_rms120_FI4_30,rmse_rms120_FI4_30,mae_rms120_FI4_30,r2_rms120_FI4_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI4_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

# Creación de dataset a partir del principal de Renta Variable Internacional, indexando la Fecha
FI_RV_Int_df_RNN_60 = FI_RV_Int_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_RV_Int_df_RNN_60.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 60 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 60 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 60 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 60 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 60 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 60 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(60,1096):
  X_train.append(training_set_escalado[i-60:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 60, todos los valores a la derecha son los valores de los 60 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 60 valores del train
test_set = training_set[-60:]
test_set = np.append(test_set ,FI_RV_Int_df_RNN_60.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_RV_Int_df_RNN_60.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(60, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-60:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la prediccion con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_RV_Int_df_RNN_60.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Renta Variable Global 60 días vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Renta Variable Global 60 días vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Renta Variable Global 60 días vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Renta Variable Global 60 días vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI4_60 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI4_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI4_60 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI4_60 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI4_60 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI4_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI4_60 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI4_60 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI4_60 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI4_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI4_60 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI4_60 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI4_60 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI4_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI4_60 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI4_60 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mse_a100_FI4_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',rmse_a100_FI4_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mae_a100_FI4_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 60 días es:',r2_a100_FI4_60)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mse_a120_FI4_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',rmse_a120_FI4_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mae_a120_FI4_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 60 días es:',r2_a120_FI4_60)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mse_rms100_FI4_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',rmse_rms100_FI4_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mae_rms100_FI4_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',r2_rms100_FI4_60)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mse_rms120_FI4_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',rmse_rms120_FI4_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mae_rms120_FI4_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',r2_rms120_FI4_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI4_60 = [[mse_a100_FI4_60,rmse_a100_FI4_60,mae_a100_FI4_60,r2_a100_FI4_60],
               [mse_a120_FI4_60,rmse_a120_FI4_60,mae_a120_FI4_60,r2_a120_FI4_60],
               [mse_rms100_FI4_60,rmse_rms100_FI4_60,mae_rms100_FI4_60,r2_rms100_FI4_60],
               [mse_rms120_FI4_60,rmse_rms120_FI4_60,mae_rms120_FI4_60,r2_rms120_FI4_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI4_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()

### **5. Fondo de Renta Variable Global: CaixaBank Global Flexible, FI**

#### *PREDICCIONES A 30 DÍAS*

# Creación de dataset a partir del principal de Renta Variable Global, indexando la Fecha
FI_RV_Glob_df_RNN_30 = FI_RV_Glob_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_RV_Glob_df_RNN_30.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 30 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 30 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 30 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 30 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 30 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 30 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(30,1096):
  X_train.append(training_set_escalado[i-30:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 30, todos los valores a la derecha son los valores de los 30 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 30 valores del train
test_set = training_set[-30:]
test_set = np.append(test_set ,FI_RV_Glob_df_RNN_30.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_RV_Glob_df_RNN_30.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(30, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-30:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la predicción con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_RV_Glob_df_RNN_30.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Renta Variable Global vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Renta Variable Global vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Renta Variable Global vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Renta Variable Global vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI5_30 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI5_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI5_30 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI5_30 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI5_30 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI5_30 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI5_30 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI5_30 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI5_30 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI5_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI5_30 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI5_30 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI5_30 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI5_30 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI5_30 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI5_30 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mse_a100_FI5_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',rmse_a100_FI5_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 30 días es:',mae_a100_FI5_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 30 días es:',r2_a100_FI5_30)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mse_a120_FI5_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',rmse_a120_FI5_30)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 30 días es:',mae_a120_FI5_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 30 días es:',r2_a120_FI5_30)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mse_rms100_FI5_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',rmse_rms100_FI5_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',mae_rms100_FI5_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 30 días es:',r2_rms100_FI5_30)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mse_rms120_FI5_30)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',rmse_rms120_FI5_30)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',mae_rms120_FI5_30)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 30 días es:',r2_rms120_FI5_30)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI5_30 = [[mse_a100_FI5_30,rmse_a100_FI5_30,mae_a100_FI5_30,r2_a100_FI5_30],
               [mse_a120_FI5_30,rmse_a120_FI5_30,mae_a120_FI5_30,r2_a120_FI5_30],
               [mse_rms100_FI5_30,rmse_rms100_FI5_30,mae_rms100_FI5_30,r2_rms100_FI5_30],
               [mse_rms120_FI5_30,rmse_rms120_FI5_30,mae_rms120_FI5_30,r2_rms120_FI5_30]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI5_30,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 30 DÍAS')
plt.show()

#### *PREDICCIONES A 60 DÍAS*

# Creación de dataset a partir del principal de Renta Variable Global, indexando la Fecha
FI_RV_Glob_df_RNN_60 = FI_RV_Glob_df

# Creacion del set de entrenamiento y converción en matriz
training_set = FI_RV_Glob_df_RNN_60.iloc[:1096, 0:1].values
training_set

# Escalado de características (Normalizacion)
sc = MinMaxScaler(feature_range=(0, 1))
training_set_escalado = sc.fit_transform(training_set)

Con esto ya hemos escalado la información

En el siguiente paso, vamos a crear estructura con 60 pasos temporales y 1 salida. Esto quiere decir que nuestro modelo va a ver los 60 precios anteriores del fondo y basado en estos precios va a intentar predecir el proximo valor de salida (el output). Se puede usar cualquier numero. 60 pasos va a corresponder con 1 mes de datos financieros

# Estructura con 60 pasos temporales y 1 salida
# Vamos a crear 2 objetos, el primero va a ser X_train (input de la red neuronal) y Y_train (output)
# Quiere decir que por cada predicción, X_train va a contener los ultimos 60 dias financieros e Y_train el siguiente dia
# vamos a inicialiarlos como listas vacias
X_train = []
y_train = []

Como queremos encontrar los últimos 60 datos, vamos a utilizar un ciclo *for* para encontrarlos

for i in range(60,1096):
  X_train.append(training_set_escalado[i-60:i,0])
  y_train.append(training_set_escalado[i,0])
X_train,y_train = np.array(X_train),np.array(y_train)

Si visualizamos la matriz X_train, vemos que la primera fila corresponde al tiempo t = 60, todos los valores a la derecha son los valores de los 60 dias previos al de la izquierda y asi sucesivamente.

X_train

len(X_train)

# Rediseño
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))
X_train

Construcción del modelo LSTM

Para la construcción, se van a tomar dos tipos de optimizadores *Adam* y *RMSprop*. Por otro lado, también se va a realizar mediante 100 y 120 iteraciones (epochs)

Optimizador *Adam*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_Adam100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam100.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_Adam = model_Adam100.fit(X_train,y_train,epochs=100,batch_size=32)

-------------------------------------------------------------------------------------

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_Adam120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_Adam120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_Adam120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_Adam120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_Adam120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_Adam120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_Adam120.compile(optimizer='adam',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_Adam = model_Adam120.fit(X_train,y_train,epochs=100,batch_size=32)

Optimizador *RMSprop*

1. Entrenamos con 100 iteraciones

# Inicialización del modelo
model_RMSprop100  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop100.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop100.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop100.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop100.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop100.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop100.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo100_RMSprop = model_RMSprop100.fit(X_train,y_train,epochs=100,batch_size=32)

2. Entrenamos con 120 iteraciones

# Inicialización del modelo
model_RMSprop120  = Sequential()

# Agregado Primera Capa LSTM y Regularizacion de Desercion
model_RMSprop120.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1))) # True es porque vamos a añadir mas capas y lo otro es porque es la primera capa
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Segunda Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Tercera Capa LSTM
model_RMSprop120.add(LSTM(units=50,return_sequences=True)) # True es porque vamos a añadir mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado Cuarta Capa LSTM (La última)
model_RMSprop120.add(LSTM(units=50)) # eliminamos return_sequences porque no vamos a agregar mas capas
model_RMSprop120.add(Dropout(0.2)) # Vamos a añadir la regularizacion de desercion 

# Agregado capa de salida - output
model_RMSprop120.add(Dense(units=1)) # 1 unidad porque solo tenemos una salida

# Optimizador y funcion de Perdida (Compilacion)
model_RMSprop120.compile(optimizer='rmsprop',loss='mean_squared_error')

# Entrenamiento del modelo red neuronal en Set de entrenamiento
modelo120_RMSprop = model_RMSprop120.fit(X_train,y_train,epochs=100,batch_size=32)

PARTE 3 - PREDICCION Y VISUALIZACION

# Creación del dataset de testeo
# Creamos nuevo array que contenga valores escalados para el test_set
# Vamos a convertir el set de datos test en matriz y le vamos a agregar los últimos 60 valores del train
test_set = training_set[-60:]
test_set = np.append(test_set ,FI_RV_Glob_df_RNN_60.iloc[1096:, 0:1].values).reshape(-1,1)

# Escalamos el array
testing_set_escalado = sc.fit_transform(test_set)
len(testing_set_escalado)

# Creación de los sets X_test e y_set
X_test = []
y_test = FI_RV_Glob_df_RNN_60.iloc[1096:, 0:1].values.reshape(-1,1)
for i in range(60, len(testing_set_escalado)):
  X_test.append(testing_set_escalado[i-60:i, 0])

# Convertimos los datos a un array con numpy
X_test = np.array(X_test)

# Reshape de los datos
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

Realizamos la prediccion con los diferentes optimizadores y los diferentes números de iteraciones

# Prediccion con Adam y 100 epochs
predictions_Adam100 = model_Adam100.predict(X_test)
predictions_Adam100 = sc.inverse_transform(predictions_Adam100)

# Prediccion con Adam y 120 epochs
predictions_Adam120 = model_Adam120.predict(X_test)
predictions_Adam120 = sc.inverse_transform(predictions_Adam120)

# Prediccion con RMS prop y 100 epochs
predictions_rmsprop100 = model_RMSprop100.predict(X_test)
predictions_rmsprop100 = sc.inverse_transform(predictions_rmsprop100)

# Prediccion con RMS prop y 120 epochs
predictions_rmsprop120 = model_RMSprop120.predict(X_test)
predictions_rmsprop120 = sc.inverse_transform(predictions_rmsprop120)

# Visualizamos
x = pd.date_range(start='1/1/2021', end='31/08/2021')
y = FI_RV_Glob_df_RNN_60.iloc[1096:, 0:1]
y['Predictions_Adam100'] = predictions_Adam100
y['Predictions_Adam120'] = predictions_Adam120
y['Predictions_rmsprop100'] = predictions_rmsprop100
y['Predictions_rmsprop120'] = predictions_rmsprop120

#Plot
fig, axs = plt.subplots(2, 2, figsize=(30,15))
axs[0, 0].plot(x, y[['VLP','Predictions_Adam100']])
axs[0, 0].set_title("Predic VLP Fondo Renta Variable Global 60 días vs VLP Real", fontsize=18)
axs[0, 0].legend(y[['VLP','Predictions_Adam100']], fontsize=16)
axs[1, 0].plot(x, y[['VLP','Predictions_Adam120']])
axs[1, 0].set_title("Predic VLP Fondo Renta Variable Global 60 días vs VLP Real", fontsize=18)
axs[1, 0].legend(y[['VLP','Predictions_Adam120']], fontsize=16)
axs[0, 1].plot(x, y[['VLP','Predictions_rmsprop100']])
axs[0, 1].set_title("Predic VLP Fondo Renta Variable Global 60 días vs VLP Real", fontsize=18)
axs[0, 1].legend(y[['VLP','Predictions_rmsprop100']], fontsize=16)
axs[1, 1].plot(x, y[['VLP','Predictions_rmsprop120']])
axs[1, 1].set_title("Predic VLP Fondo Renta Variable Global 60 días vs VLP Real", fontsize=18)
axs[1, 1].legend(y[['VLP','Predictions_rmsprop120']], fontsize=16)
fig.tight_layout()

# Calculos de indicadores (MSE, RMSE, MAE y R2) para cada red Neuronal

# Calculo indicadores Red Optimizador Adam - 100 Epochs
mse_a100_FI5_60 = round(mean_squared_error(y_test,predictions_Adam100),5)
rmse_a100_FI5_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam100)),5)
mae_a100_FI5_60 = round(mean_absolute_error(y_test,predictions_Adam100),5)
r2_a100_FI5_60 = round(r2_score(y_test,predictions_Adam100),5)

# Calculo indicadores Red Optimizador Adam - 120 Epochs
mse_a120_FI5_60 = round(mean_squared_error(y_test,predictions_Adam120),5)
rmse_a120_FI5_60 = round(sqrt(mean_squared_error(y_test,predictions_Adam120)),5)
mae_a120_FI5_60 = round(mean_absolute_error(y_test,predictions_Adam120),5)
r2_a120_FI5_60 = round(r2_score(y_test,predictions_Adam120),5)

# Calculo indicadores Red Optimizador RMSprop - 100 Epochs
mse_rms100_FI5_60 = round(mean_squared_error(y_test,predictions_rmsprop100),5)
rmse_rms100_FI5_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop100)),5)
mae_rms100_FI5_60 = round(mean_absolute_error(y_test,predictions_rmsprop100),5)
r2_rms100_FI5_60 = round(r2_score(y_test,predictions_rmsprop100),5)

# Calculo indicadores Red Optimizador RMSprop - 120 Epochs
mse_rms120_FI5_60 = round(mean_squared_error(y_test,predictions_rmsprop120),5)
rmse_rms120_FI5_60 = round(sqrt(mean_squared_error(y_test,predictions_rmsprop120)),5)
mae_rms120_FI5_60 = round(mean_absolute_error(y_test,predictions_rmsprop120),5)
r2_rms120_FI5_60 = round(r2_score(y_test,predictions_rmsprop120),5)

# Print de resultados
# Resultados Red Optimizador Adam - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mse_a100_FI5_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',rmse_a100_FI5_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 100 Epochs de 60 días es:',mae_a100_FI5_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 100 Epochs de 60 días es:',r2_a100_FI5_60)

# Resultados Red Optimizador Adam - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mse_a120_FI5_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',rmse_a120_FI5_60)
print('El Error Absoluto Medio para la Red Optimizador Adam - 120 Epochs de 60 días es:',mae_a120_FI5_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador Adam - 120 Epochs de 60 días es:',r2_a120_FI5_60)

# Resultados Red Optimizador RMSprop - 100 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mse_rms100_FI5_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',rmse_rms100_FI5_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',mae_rms100_FI5_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 100 Epochs de 60 días es:',r2_rms100_FI5_60)

# Resultados Red Optimizador RMSprop - 120 Epochs
print('El Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mse_rms120_FI5_60)
print('La Raíz del Error Cuadrático Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',rmse_rms120_FI5_60)
print('El Error Absoluto Medio para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',mae_rms120_FI5_60)
print('El Coeficiente de Determinación (R2) para la Red Optimizador RMSprop - 120 Epochs de 60 días es:',r2_rms120_FI5_60)

plt.rcParams["figure.figsize"] = [7.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots(1, 1)
data_FI5_60 = [[mse_a100_FI5_60,rmse_a100_FI5_60,mae_a100_FI5_60,r2_a100_FI5_60],
               [mse_a120_FI5_60,rmse_a120_FI5_60,mae_a120_FI5_60,r2_a120_FI5_60],
               [mse_rms100_FI5_60,rmse_rms100_FI5_60,mae_rms100_FI5_60,r2_rms100_FI5_60],
               [mse_rms120_FI5_60,rmse_rms120_FI5_60,mae_rms120_FI5_60,r2_rms120_FI5_60]]
column_labels = ["MSE", "RMSE", "MAE", "R2"]
axs.axis('tight')
axs.axis('off')
df=pd.DataFrame(data_FI5_60,columns=column_labels)
the_table = axs.table(cellText=df.values,
                      colLabels=df.columns,
                      rowLabels=["Adam - 100 Epochs","Adam - 120 Epochs","RMSprop - 100 Epochs","RMSprop - 120 Epochs"],
                      rowColours =["yellow"] * 4,  
                      colColours =["yellow"] * 4,
                      loc="center",
                      )
the_table.auto_set_font_size(False)
the_table.set_fontsize(12)
the_table.scale(2,3)
plt.title('MODELOS DE REGRESIÓN - PREDICCIÓN 60 DÍAS')
plt.show()
